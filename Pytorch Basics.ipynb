{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7414e+16, 3.1434e-12, 8.0775e+17],\n",
      "        [1.4583e-19, 4.1486e-08, 1.3556e-19],\n",
      "        [1.8567e-01, 1.2043e-32, 1.3563e-19],\n",
      "        [1.8888e+31, 4.7414e+16, 4.0047e-11],\n",
      "        [6.4097e-10, 5.8253e-10, 6.4097e-10]])\n",
      "tensor([[0.3982, 0.7172, 0.2691],\n",
      "        [0.7341, 0.4943, 0.0773],\n",
      "        [0.3557, 0.5978, 0.7238],\n",
      "        [0.8451, 0.2157, 0.1492],\n",
      "        [0.8975, 0.4766, 0.5946]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.5598,  0.2589, -1.4018],\n",
      "        [ 0.0256,  1.0924, -1.0235],\n",
      "        [-1.3870,  0.2484,  2.9512],\n",
      "        [-0.1714,  0.6173, -1.5169],\n",
      "        [-0.9064, -0.0053, -1.6680]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "\n",
    "#uninitialised matrix of 5*3 in pytorch\n",
    "x=torch.empty(5,3)\n",
    "print(x)\n",
    "\n",
    "#randomly uninitialised 5*3 matrix\n",
    "x=torch.rand(5,3)\n",
    "print(x)\n",
    "\n",
    "#matrix of zeroes and of the datatype long\n",
    "x=torch.zeros(5,3,dtype=torch.long)\n",
    "print(x)\n",
    "\n",
    "x=torch.tensor([5.5,3])\n",
    "print(x)\n",
    "\n",
    "#create a new tensor based on an existing tensor\n",
    "x=x.new_ones(5,3,dtype=torch.double)\n",
    "print(x)\n",
    "#same dimensions as x but overriding the dtype\n",
    "x=torch.randn_like(x,dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "#to print the size of the tensor\n",
    "print(x.size())\n",
    "#torch.size is a tuple and it supports all tuple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations in Pytorch\n",
    "- Addition - two possible\n",
    "- an operation which mutates the tensor in place is postfixed with _ x.copy_() will change x. Simlarly other in place operations\n",
    "- To resize a tensor use (tensor).view\n",
    "- Other tensor operations https://pytorch.org/docs/stable/torch.html\n",
    "- All tensors except the chartensor can be converted to the Numpy and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3189,  0.1046,  0.3333],\n",
      "        [ 0.5114,  0.7004, -0.4639],\n",
      "        [-0.1431,  0.3312,  0.7110],\n",
      "        [-0.0720, -0.6239,  1.1720],\n",
      "        [-0.8311,  1.6144,  0.0330]])\n",
      "tensor([[ 2.3189,  0.1046,  0.3333],\n",
      "        [ 0.5114,  0.7004, -0.4639],\n",
      "        [-0.1431,  0.3312,  0.7110],\n",
      "        [-0.0720, -0.6239,  1.1720],\n",
      "        [-0.8311,  1.6144,  0.0330]])\n",
      "tensor([[ 2.3189,  0.1046,  0.3333],\n",
      "        [ 0.5114,  0.7004, -0.4639],\n",
      "        [-0.1431,  0.3312,  0.7110],\n",
      "        [-0.0720, -0.6239,  1.1720],\n",
      "        [-0.8311,  1.6144,  0.0330]])\n",
      "tensor([[ 2.3189,  0.1046,  0.3333],\n",
      "        [ 0.5114,  0.7004, -0.4639],\n",
      "        [-0.1431,  0.3312,  0.7110],\n",
      "        [-0.0720, -0.6239,  1.1720],\n",
      "        [-0.8311,  1.6144,  0.0330]])\n",
      "tensor([[ 2.3189,  0.1046,  0.3333],\n",
      "        [ 0.5114,  0.7004, -0.4639],\n",
      "        [-0.1431,  0.3312,  0.7110],\n",
      "        [-0.0720, -0.6239,  1.1720],\n",
      "        [-0.8311,  1.6144,  0.0330]])\n"
     ]
    }
   ],
   "source": [
    "#Operations of Pytorch\n",
    "\n",
    "y=torch.rand(5,3)\n",
    "print(x+y)  # one of the syntaxes of addition\n",
    "print(torch.add(x,y)) #second syntax for addition\n",
    "\n",
    "#output as a tensor\n",
    "result=x+y\n",
    "print(result)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)\n",
    "\n",
    "#in place addition - adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4389, -0.3004, -0.0874],\n",
      "        [-0.2956, -0.1454, -0.7984],\n",
      "        [-0.7909, -0.1825, -0.1275],\n",
      "        [-0.2338, -1.3596,  0.6850],\n",
      "        [-1.3156,  0.8451, -0.9145]])\n",
      "tensor([-0.3004, -0.1454, -0.1825, -1.3596,  0.8451])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[:,1]) #prints the column at index 1 (i.e. 2nd column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2086, -0.2047, -0.1165, -0.5127],\n",
      "        [ 0.2755,  1.7972, -0.4807, -0.0735],\n",
      "        [-0.6559,  2.6421,  0.5717,  1.9044],\n",
      "        [-0.3256,  0.0643, -0.2601, -1.0877]])\n",
      "torch.Size([16])\n",
      "tensor([[-0.2086, -0.2047, -0.1165, -0.5127,  0.2755,  1.7972, -0.4807, -0.0735],\n",
      "        [-0.6559,  2.6421,  0.5717,  1.9044, -0.3256,  0.0643, -0.2601, -1.0877]])\n",
      "torch.Size([2, 8])\n",
      "tensor([1.0554])\n",
      "1.0554451942443848\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(4,4)\n",
    "print(x)\n",
    "y=x.view(16)\n",
    "print(y.size())\n",
    "z=x.view(-1,8) #the size -1 is inferred from other dimensions (dunno what this means)\n",
    "print(z)\n",
    "print(z.size())\n",
    "\n",
    "#if you have a one element tensor, use .item to the get the value as the python number\n",
    "x=torch.randn(1)\n",
    "print(x) #this prints as a matrix\n",
    "print(x.item()) #this prints as a python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#converting torch tensor to a numpy array\n",
    "a=torch.ones(5)\n",
    "print(a)\n",
    "b=a.numpy()\n",
    "print(b)\n",
    "\n",
    "#operations done on a will change the numpy array b also\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "#converting numpy array to a torch tensor\n",
    "import numpy as np\n",
    "a=np.ones(5)\n",
    "print(a)\n",
    "b=torch.from_numpy(a)\n",
    "print(b)\n",
    "np.add(a,1,out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd Package\n",
    "- Central to all neural networks in Pytorch is the autograd package\n",
    "- autograd package provides automatic differentiation for all operations on tensors\n",
    "- Define-by-run framework\n",
    "    - this means that my backprop is defined by how my code is run, and that every single iteration can be different\n",
    "\n",
    "### Tensor\n",
    "- torch.tensor is the central class of the package\n",
    "- if the attribute of the class torch.tensor, .requires_grad as True, then it starts to track all the operations on it (on the attribute?)\n",
    "- .backward() can be called after the operations to calculate the gradients automatically. The gradient for this tensor will be accumulated into the .grad attribute\n",
    "- To stop a tensor from calling history you can call the .detach() to detach it from the computation history\n",
    "- to prevent tracking history wrap the code block in \"with torch.no_grad():\"\n",
    "- Function class is also important for autograd implementation\n",
    "- Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation\n",
    "    - Each tensor has a .grad_fn attribute that references a function that has created the tensor\n",
    "- to compute the derivatives, call backward function on a tensor\n",
    "    - if a tensor is scalar,then no arguments for the the backward function\n",
    "    - tensor is non-scalar then speficy a gradient argument that is a tensor of matching shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "<AddBackward0 object at 0x7fb2187eaeb0>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7fb2187ea8b0>\n"
     ]
    }
   ],
   "source": [
    "x=torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y=x+2 #adds 2 to every element of the tensor\n",
    "print(y.grad_fn)\n",
    "\n",
    "z=y*y*3\n",
    "out=z.mean() #returns the mean value of each dimension of the tensor\n",
    "print(z,out)\n",
    "\n",
    "#requires_grad_( ... ) changes an existing Tensor’s requires_grad flag in-place. The input flag defaults to False if not given.\n",
    "\n",
    "a=torch.randn(2,2)\n",
    "a=((a*3)/(a-1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b=(a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "<AddBackward0 object at 0x7fb28c2daeb0>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "#GRADIENTS\n",
    "x=torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y=x+2 #adds 2 to every element of the tensor\n",
    "print(y.grad_fn)\n",
    "\n",
    "z=y*y*3\n",
    "out=z.mean() #returns the mean value of each dimension of the tensor\n",
    "print(z,out)\n",
    "\n",
    "out.backward(retain_graph=True) #out contains a single scalar so out.backward is equivalent to out.backward(torch.tensor(1,))\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2430,  1.2260,  2.2465], grad_fn=<MulBackward0>)\n",
      "tensor([2.0000e-01, 2.0000e+00, 2.0000e-04])\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "tensor(True)\n",
      "tensor([-0.1215,  0.6130,  1.1233], requires_grad=True)\n",
      "tensor([-0.1215,  0.6130,  1.1233])\n"
     ]
    }
   ],
   "source": [
    "#torch.autograd is an engine for computing vector-Jacobian product\n",
    "#the characteristic of a vector Jacobian product makes it very convenient to feed external gradients to a model that has non scalar output\n",
    "x=torch.randn(3,requires_grad=True)\n",
    "y=x*2\n",
    "print(y)\n",
    "v=torch.tensor([0.1,1.0,0.0001],dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)\n",
    "\n",
    "#to stop tracking history with tensors use .requires_grad=True or enclose the code block with torch.no_grad()\n",
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)\n",
    "    \n",
    "print((x**2).requires_grad)\n",
    "\n",
    "#using detach get a new tensor with the same content but that does not require gradients\n",
    "print(x.requires_grad)\n",
    "y=x.detach()\n",
    "print(y.requires_grad)  #by default the grad is false\n",
    "print(x.eq(y).all())\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"[The trio] realizes they’re never going to be able to feel relief. They’re always going to feel the stress, the fear of Voldemort coming closer, being ever present, being such a danger.\"\n",
    "- Casper on relief in Book 7, Chapter 9\n",
    "\n",
    "\"Oh, damn, that'll be the books,\" Hermione said, peering into it, \"and I had them all stacked by subject...\"\n",
    "-Chapter 9 - when we're introduced to Hermione's bag with its undetectable extension charm!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
